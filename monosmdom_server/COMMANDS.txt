# Import an OSM link dump:
./manage.py update_osm_state ~/all_20231101.monosmdom.json
# Or if you want to skip the sanity-check screen:
./manage.py update_osm_state ~/all_20231101.monosmdom.json --force OVERWRITE

# Run the interactive web server (which does not crawl):
gunicorn --error-logfile - --reload --bind 0.0.0.0:8000 monosmdom_server.wsgi:application

# Run the crawler (which does not provide any useful means of interacting with the data directly, just dumps it in the DB)
# Single-shot:
./manage.py dbcrawl --url "https://specific.simplified.url/that/appears/verbatim/in/the/DB"
./manage.py dbcrawl --domain specific.verbatim.existing.domain
./manage.py dbcrawl --random-url
# Automatically keep crawling:
./manage.py dbcrawl --random-url --next-delay-seconds 1

# Check whether curl even works (does NOT write anything to the database!)
./manage.py minicrawl "https://example.com"

cd /path/to/monosmdom_media/res/
# Find worst causes of duplicates:
find -type f | xargs sha256sum | sort | uniq -cdw16 | sort -n | tail

# Clear FileField cells in the database that point to deleted files.
# (For example, those that you deleted as part of a cleanup after the previous duplicate-detection.)
./manage.py clear_deleted

# Download database:
ssh myserver pg_dump --verbose --clean --no-acl --no-owner --if-exists monosmdom > myserver_monosmdom_pg_dump.psql
# Download (add-only) crawl results. Trailing slashes must be exactly like this:
rsync --stats -av myserver:/path/to/monosmdom_media/res/ /path/to/local/monosmdom_media/res
# Import database:
dropdb monosmdom_mirror
createdb monosmdom_mirror
psql --set ON_ERROR_STOP=on monosmdom_mirror < myserver_monosmdom_pg_dump.psql
